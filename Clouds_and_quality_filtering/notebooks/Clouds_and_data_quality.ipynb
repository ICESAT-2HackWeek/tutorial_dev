{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data_dir='ATL06/Byrd_glacier/'\n",
    "\n",
    "# NOTE: We'll be working in matplotlib's widget mode, which lets us zoom in on our plots.  \n",
    "# This means that the figures won't be rendered in the notebook until you run them.\n",
    "# That means no spoilers (for now...)\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clouds over land ice: What problems they cause, and what to do about it.\n",
    "This tutorial covers some of the reasons you might see weird results over ice when clouds start to blot out the surface signals.  The learning objectives I'd like to get to are:\n",
    "- Understanding how clouds affect laser-altimetry signals\n",
    "- Recognizing how these effects are manifest in the ATL06 product\n",
    "- Gaining familiarity with the ATL06 parameters that can identify cloudy returns\n",
    "This part of the tutorial will focus on clouds that cause gross errors in surface-height estimates.  \n",
    "\n",
    "Along the way, we'll:\n",
    "\n",
    "- develop a simple function for reading ICESat-2 data from hdf-5 files\n",
    "\n",
    "If time allows, I'll also present on subtler effects caused by forward scattering of laser light by thin clouds, which can lead to smaller elevation biases.  The objectives of this part of the tutorial will be:\n",
    "- Understanding how forward scattering of laser light by clouds can introduce biases in surface-height estimates\n",
    "- Tools for identifying returns affected by forward scattering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.  Introduction: ATL06 review\n",
    "\n",
    "<img src=\"images/ATL06_segment_model.tiff\"  width=600 height=600>\n",
    "\n",
    "Recall that ATL06 gives us surface heights based on the heights of collections of photons in a 40-m (horizontal) by w_surface_window (vertical) window.  It uses a variety of techniques to shrink the window to the smallest size that contains the surface without clipping off any signal photons.  In developing the algorithms, we decided that reporting some bad elevations was better than throwing away good elevations, and as a result, an ATL06 data file includes some bad returns, sometimes quite a few.  We're going to explore why, and how to tell the good from the bad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. An oversimplified desciption of snow and cloud radiative transfer\n",
    "\n",
    "Clouds and snow reflect light in more or less the same way.  Light hits an ice grain and scatters into a random direction.  This happens over an over until the light is either absorbed or scattered out of the layer.  The number of expected scattering and absorption events per unit distance is called the _extinction coefficient_ ($\\mu_e$}, and it depends on the size and density of the snow grains or ice grains doing the scattering.  For green light, absorption of light by ice isn't significant, and we're going to ignore it.  Because ice and snow scatter light mostly in the forward direction, it takes about 8 scattering events to turn a photon entirely around, so if we calculate (or read) a value for an extinction coefficient, we can scale it by $1/S$ and approximate the scattering process as isotropic (meaning that the photon can go in any direction with equal probability). A typical value for $S$ from the literature is 6-10, so we're going to use $S=10$. We're going to make a really simple model of light interacting with a cloud and an ice-sheet surface and see what the expected returns look like.\n",
    "\n",
    "We can describe the beam of light going down from ICEsat-2 as a collection of $N_0$ photons.  When the light hits a cloud (or snow) layer whose top is at $z_0$, the photons are attenuated by the cloud such that:\n",
    "$$dN(z)/dz = -\\frac{\\mu_s}{S} N(z)$$\n",
    "and  the rate of photons scattered isotropically is:\n",
    "$$N_S(z) = \\frac{\\mu_s}{S} N(z)$$\n",
    "We're going to make the approximation that isotropically scattered photons don't interact with the layer again.  This is a good approximation for optically thin layers, and a bad one for thick layers, but we can get some intuition for pulse shapes even with this baggage.  \n",
    "\n",
    "For a fine-grained, flat snow surface, all of the downgoing photons will be scattered over a short depth, $\\delta$, so \n",
    "$$\\int^{z_s}_{z_s-\\delta} N_S=N_0$$\n",
    "We know that in this situation, ICEsat-2 will detect about 3 photons/pulse (for a weak beam), or about 12 photons/pulse for a strong beam.  We can use this to approximate a return profile as:\n",
    "$$N_d(z, z+\\delta z) = N_e e^{-\\tau(z)}\\mu_s(z)\\delta z$$\n",
    "where $N_e$=12 for a strong beam $N_e$=3 for a weak beam, and $\\tau(z)$ is the optical depth at $z$, defined:\n",
    "$$\\tau(z) = \\int^\\infty_z\\mu_s(z) dz.$$\n",
    "\n",
    "Because ICESat-2 operates both day and night, and because cloud surfaces and snow are both good reflectors, a lot of solar background photons come into the telescope along with the signal photons.  There are fiters that remove most of these photons, but a few of them make it through, at rates between ~0 and ~12 MHz.  In a vertical window $\\delta_z$ wide, this gives $\\frac{R_{BG}}{c/2} \\delta_z$ photons.\n",
    "\n",
    "\n",
    "This gives us everything we need to generate synthetic estimates of cloud returns.  Here's a quick Python function that generates the probability of a photon return in a series of height increments:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def return_P(n0, z0, mu_s, BGR=0):\n",
    "    \"\"\"\"\n",
    "        Calculate the expected return rate for an ICESat-2 return \n",
    "        \n",
    "        Inputs:\n",
    "            z0: layer edges for cloud layers, in height, low to high (vector, n+1)\n",
    "            mu_s: scattering coefficient (/m) for layers (vector, n)\n",
    "            BGR: Background rate, in Hz\n",
    "        outputs:\n",
    "            z: layer center heights (meters, vector[n])\n",
    "            P: probability of a photon returned from each layer (unitless, vector[n])\n",
    "            tau: optical depth to the bottom of each layer\n",
    "    \"\"\"\n",
    "    # reverse the inputs to be ITO depth\n",
    "    depth=-z0[::-1]\n",
    "    d_depth=np.diff(depth)\n",
    "    mu_sr=mu_s[::-1]\n",
    "    # optical depth at the layer boundaries\n",
    "    tau=np.concatenate([[0], np.cumsum(d_depth*mu_sr)])\n",
    "    # number of photons left in the beam at the layer boundaries\n",
    "    N=n0*np.exp(-tau)\n",
    "    # number of photons emitted in each layer\n",
    "    P=-np.diff(N)\n",
    "    z=(z0[0:-1]+z0[1:])/2\n",
    "    # Background photon rate:\n",
    "    N_BG=BGR/1.5e8*d_depth\n",
    "    return z, P[::-1]+N_BG[::-1], tau[::-1]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Simulated returns from clouds and snow : signal strength\n",
    "\n",
    "Let's see what the expected number of photons returned from a cloud top and a snow layer looks like:  A thick cloud could have $\\mu_s$=0.005, and a loosely packed snow layer could have $\\mu_s$=500.  We'll model the cloud and the snow surface as a step change in $\\mu_s$ at z=0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f3327d76264da8b407ae65fec36762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'return density, $ph/m/pulse$')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz=0.2\n",
    "z0=np.arange(-20, 10, dz)\n",
    "zc, Pc, tau_c = return_P(12, z0, (z0[0:-1]<0).astype(np.float64)*0.01)\n",
    "zs, Ps, tau_s = return_P(12, z0, (z0[0:-1]<0).astype(np.float64)*500)\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.plot(Pc/dz, zc, label='cloud')\n",
    "plt.title('cloud')\n",
    "plt.xlabel('return density, $m^{-1}$')\n",
    "plt.ylabel('height, m')\n",
    "plt.subplot(122)\n",
    "plt.plot(Ps/dz, zs, label='snow')\n",
    "plt.title('snow')\n",
    "plt.xlabel('return density, $ph/m/pulse$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The peak return density from the cloud is around 0.12 photons/meter/pulse, while the return density from the snow is around 120 photons/meter/pulse (higher actually, but we're avearging in decimeter increments). This shows that under clear conditions, snow are extremely sharp, and cloud returns are much weaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Simluated returns from snow under clouds\n",
    "\n",
    "If only one layer is present there's no ambiguity what might be a cloud and what might be a snow surface.  The problem is harder when there's a cloud on top of snow.  Let's define a function to calculate scattering coefficients for a cloud and a snow layer, and use it to plot return densities for different cloud thicknesses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5593d6e0e51d444698211d7bbbe5195f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mu_profile(z0, z_cloud_top, z_surf=0, mu_cloud=0.005, mu_surf=500):\n",
    "    zc=(z0[:-1]+z0[1:])/2\n",
    "    mu_s=np.ones_like(zc)+np.NaN\n",
    "    # make everything below zc=z_surf snow:\n",
    "    mu_s[zc <= z_surf]=mu_surf\n",
    "    #make everything between zc=0 and zc=z_cloud_top a cloud:\n",
    "    mu_s[ zc >= z_surf ] = mu_cloud\n",
    "    mu_s[ zc >= z_cloud_top ] =0\n",
    "    return mu_s\n",
    "\n",
    "# now make some plots\n",
    "dz=0.2\n",
    "# height domain is between -20 and 1100 m\n",
    "z0=np.arange(-200, 2200, dz)\n",
    "\n",
    "# we'll save these results for later.\n",
    "P_save={}\n",
    "\n",
    "fig=plt.figure()\n",
    "for axes_ind, z_cloud_top in enumerate([20, 500, 1000, 1500]):\n",
    "    mu_p=mu_profile(z0, z_cloud_top, mu_cloud=0.005, mu_surf=500)\n",
    "    zc, P_profile, tau = return_P(12, z0, mu_p)\n",
    "    N_surf=np.sum(P_profile[np.abs(zc)<10])\n",
    "    fig.add_subplot(1, 4, axes_ind+1)\n",
    "    plt.plot(P_profile, zc)\n",
    "    plt.xlabel('ph/pulse/20cm')\n",
    "    plt.title('$z_{top}$= %d m\\n N_surf=%2.2f' % (z_cloud_top, N_surf))\n",
    "    # save the results:\n",
    "    P_save[z_cloud_top]=P_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the scaling on the plots obscures this, the peak amplitude of the cloud return is consistent at 0.012 photons/20 cm bin, while the surface return gets weaker as the cloud gets thicker.  At some point, the surface-finding algorithms no longer pick up the surface, but may pick up the cloud top instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Calculating the probability of a false detection\n",
    "\n",
    "ATL06 is based on segments of data 40 m long, which usually contain data from 57 pulses.  It only identifies returns with at least 10 photons, and aggregates photons over vertical windows between 3 and 20 meters wide (with the width dependent on the segment slope).  Let's calculate the probability of ATL06 identifying the surface when we aggregate photons over 20 meters horizontally, 3 m vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b17ffd4fba403b886f54577118fcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as sps\n",
    "\n",
    "def aggregate(z, P, N_horz, W):\n",
    "    P_agg=np.zeros_like(P)\n",
    "    for ii, zi in enumerate(z):\n",
    "        els=np.abs(z-zi) <= W/2\n",
    "        P_agg[ii]=N_horz*np.sum(P[els])\n",
    "    return P_agg\n",
    "\n",
    "fig=plt.figure(); plt.clf()\n",
    "for ax_num, z_cloud_top in enumerate(P_save.keys()):\n",
    "    P1=aggregate(zc, P_save[z_cloud_top], 57, 3)\n",
    "    \n",
    "    P_10ph = 1-sps.poisson.cdf(10, P1)\n",
    "    P_surf=np.max(P_10ph[zc < 50])\n",
    "    fig.add_subplot(1, 4, ax_num+1)\n",
    "    plt.plot(P_10ph, zc)\n",
    "    plt.title('$z_{top}$= %d m\\n $P_{surf}=%1.3f$' % (z_cloud_top, P_surf))\n",
    "    plt.xlabel('$P_{10 ph.}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that there's a large vertical region just below the top of the cloud where it's very easy to find windows containing 10 photons or more.  In these cases, we're counting on the surface return to be stronger than the cloud return, but if the cloud is thick enough to obscure the surface, it's easy to find returns higher in the atmosphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise for participants: Explore: \n",
    "# what happens when the bacground rate is larger?  Try making these plots with BGR = 10 MHz.  What do you see?\n",
    "\n",
    "#What happens when the vertical window size is larger?  Try making these plots with W=10 or W=20.  What do you see?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations that (I hope) some of you will have made:\n",
    "1.  Once the background rate becomes large (5 MHz+) it's pretty common to see large regions where there might be a false surface detection\n",
    "2. For large windows, the region where we might see false surface detections is much larger, and isn't just restricted to the cloud top and the surface\n",
    "\n",
    "Based on these observations, we should be prepared to see a lot of false surface detections, and we should expect worse results in cases where the surface window has not converged to a small value.  However, if we see large numbers of photons, especially concentrated in a small window, we should be prepeared to believe that they are from the surface!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Relating cloud effects to ATL06 data parameters\n",
    "\n",
    "Our problem comes when the ground return is not strong enough to trigger the signal finder, and we start to see triggers associated with:\n",
    "- Cloud tops\n",
    "- Random clusterings of background photons\n",
    "\n",
    "These should both be statistically distinct from surface returns because:\n",
    "- The returns are less intense than a high-quality surface return\n",
    "- The photons are more widely vertcally spread than those in surface returns\n",
    "- The surface window cannot converge on a small vertical window around the surface\n",
    "- Surface heights and slopes are not consistent between adjacent segments\n",
    "\n",
    "There are a few ATL06 parameters that help quantify these distinctions. \n",
    "In the /gtxx/land_ice_segments group:\n",
    "- h_li_sigma : the estimated error in the surface-height estimate\n",
    "In the /gtxx/land_ice_segments/fit_statistics groups:\n",
    "- n_fit_photons : The number of photons found in each segment\n",
    "- w_surface_window_final : The size of the converged surface window\n",
    "- h_rms_misft : The RMS misfit of photons in the surface window\n",
    "- h_robust_sprd : A percentile-based estimate of the spread of the photons, corrected for background\n",
    "- snr : the observed signal-to-noise ratio for the selected photons\n",
    "- snr_significance : The estimated probability that a random clustering of photons would produce the observed SNR\n",
    "- dh_fit_dx : the along-track segment slope\n",
    "And in /gtxx/land_ice_segments/geophysical:\n",
    "- r_eff : the effective reflectance of the surface\n",
    "\n",
    "There's one more parameter that puts a few of these ideas together, in /gtxx/land_ice_segments:\n",
    "- atl06_quality_summary : a combination of parameters (h_li_sigma, n_fit_photons/w_surface_window_final, and signal_selection_source).  Zero indicates a good segment, 1 indicates a possibly bad segment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Taking our picture of cloud effects to the ice sheet\n",
    "We're next going to look at data from Byrd Glacier, in Antarctica.  This is one of the big outlet glaciers dumping ice from the East Antarctic plateau into the Ross Ice Shelf.  Here's a shaded-relief map from the REMA.\n",
    "<img src=\"https://www.gislounge.com/wp-content/uploads/2018/09/mulock-glacier-dem.jpg\">\n",
    "\n",
    "The glacier flows west to east (the REMA image is south-end-up, so east is on the left).  The inland catchment of the glacier should have a smooth ice surface, but the glacier trunk and the area where it joins the ice shelf are heavily crevassed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've pulled the ICESat-2 data for the Byrd catchment, and it's stored on S3, so you can grab it with these commands:\n",
    "\n",
    "    mkdir /home/jovyan/ATL06\n",
    "    aws s3 cp s3://pangeo-data-upload-oregon/icesat2/Clouds_and_filtering_tutorial/ /home/jovyan/ATL06/Byrd_glacier --recursive\n",
    "        \n",
    "Once that has finished downloading, take a look at what's there, using some basic python hdf5 code.  This takes about a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 250 beam/pair combinations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca425b4c6a3425485024fcf521201ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f1f397a3a58>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir='/home/jovyan/ATL06/Byrd_glacier/'\n",
    "D6=[]\n",
    "pairs=[1, 2, 3]\n",
    "beams=['l','r']\n",
    "datasets=['latitude','longitude','h_li','atl06_quality_summary', 'x_atc','y_atc']\n",
    "files=glob(data_dir+'/*.h5')\n",
    "for ii, file in enumerate(files):\n",
    "    this_name=os.path.basename(file)\n",
    "    with h5py.File(file) as h5f:\n",
    "        for pair in pairs:\n",
    "            for beam in beams:\n",
    "                temp={}\n",
    "                for dataset in datasets:\n",
    "                    DS='/gt%d%s/land_ice_segments/%s' % (pair, beam, dataset)\n",
    "                    try:\n",
    "                        temp[dataset]=np.array(h5f[DS])\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                if len(temp) > 0:\n",
    "                    D6.append(temp)\n",
    "print(\"read %d beam/pair combinations\" % (len(D6)))\n",
    "\n",
    "# now plot the results:\n",
    "plt.figure();\n",
    "for Di in D6:\n",
    "    plt.scatter(Di['longitude'], Di['latitude'], c=Di['h_li'], vmin=0, vmax=2000, linewidth=0)\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is opposite the orientation of the REMA map, but we can see the trunk of the glacier (dark blue, right-center) and we can see the plateau (bright yellow, at left).\n",
    "\n",
    "It looks like some of those tracks worked really well, and some of them didn't.  It's not all that much fun typing in all that code every time we want to read an ATL06 file.  Let's make a function to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATL06_to_dict(filename, dataset_dict):\n",
    "    \"\"\"\n",
    "        Read selected datasets from an ATL06 file\n",
    "        \n",
    "        Input arguments:\n",
    "            filename: ATl06 file to read\n",
    "            dataset_dict: A dictinary describing the fields to be read\n",
    "                    keys give the group names to be read, \n",
    "                    entries are lists of datasets within the groups\n",
    "        Output argument:\n",
    "            D6: a list of dictionaries containing ATL06 data.  Each dataset in \n",
    "                dataset_dict has its own entry in each element of D6.\n",
    "    \"\"\"\n",
    "    D6=[]\n",
    "    pairs=[1, 2, 3]\n",
    "    beams=['l','r']\n",
    "\n",
    "    with h5py.File(file) as h5f:\n",
    "        for pair in pairs:\n",
    "            for beam_ind, beam in enumerate(beams):\n",
    "                temp={}\n",
    "                for group in dataset_dict.keys():\n",
    "                    for dataset in dataset_dict[group]:\n",
    "                        DS='/gt%d%s/%s/%s' % (pair, beam, group, dataset)\n",
    "                        try:\n",
    "                            temp[dataset]=np.array(h5f[DS])\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "                if len(temp) > 0:\n",
    "                    temp['pair']=np.zeros_like(temp['h_li'])+pair\n",
    "                    temp['beam']=np.zeros_like(temp['h_li'])+beam_ind\n",
    "                    temp['filename']=filename\n",
    "                    D6.append(temp)\n",
    "    return D6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop over the files and start to get an idea of which ones are the good ones and which ones are the bad ones. Let's look at the first five files, plotting the surface height as a function of x_atc for the first beam/pair combination (gt1l):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b00e31d6e14c3cab333e6f343d828e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c0e5cdd984439e9fd47cb645a57bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e89f79bcf97416ca67d31804947cfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first define a dataset dictionary.  This tells our script what fields to read and where to find them\n",
    "dataset_dict={'land_ice_segments': ['h_li','latitude','longitude'],\n",
    "             'land_ice_segments/ground_track':['x_atc']}\n",
    "\n",
    "for file in files[0:3]:\n",
    "    this_D6=ATL06_to_dict(file, dataset_dict)\n",
    "    plt.figure()\n",
    "    plt.plot(this_D6[0]['x_atc'], this_D6[0]['h_li'],'.')\n",
    "    plt.title(os.path.basename(this_D6[0]['filename']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look good either- lots of those elevations are on the order of $10^{38}$ m, which is a bit high for Antarctica.  Our function is doing something dumb: In fact, it's not handling no-data values correctly.  We need to read the '\\_FillValue' attribute for field, and set the elements with that value to NaN.  Here's a fixed version of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATL06_to_dict(filename, dataset_dict):\n",
    "    \"\"\"\n",
    "        Read selected datasets from an ATL06 file\n",
    "\n",
    "        Input arguments:\n",
    "            filename: ATl06 file to read\n",
    "            dataset_dict: A dictinary describing the fields to be read\n",
    "                    keys give the group names to be read, \n",
    "                    entries are lists of datasets within the groups\n",
    "        Output argument:\n",
    "            D6: dictionary containing ATL06 data.  Each dataset in \n",
    "                dataset_dict has its own entry in D6.  Each dataset \n",
    "                in D6 contains a list of numpy arrays containing the \n",
    "                data\n",
    "    \"\"\"\n",
    "    D6=[]\n",
    "    pairs=[1, 2, 3]\n",
    "    beams=['l','r']\n",
    "\n",
    "    with h5py.File(filename) as h5f:\n",
    "        for pair in pairs:\n",
    "            for beam_ind, beam in enumerate(beams):\n",
    "                # check if a beam exists, if not, skip it\n",
    "                if '/gt%d%s/land_ice_segments' % (pair, beam) not in h5f:\n",
    "                    continue\n",
    "                temp={}\n",
    "                for group in dataset_dict.keys():\n",
    "                    for dataset in dataset_dict[group]:\n",
    "                        DS='/gt%d%s/%s/%s' % (pair, beam, group, dataset)\n",
    "                        # some parameters have a _FillValue attribute.  If it exists, use it to identify bad values, and set them to np.NaN\n",
    "                        try:\n",
    "                            temp[dataset]=np.array(h5f[DS])\n",
    "                            if '_FillValue' in h5f[DS].attrs:\n",
    "                                fill_value=h5f[DS].attrs['_FillValue']\n",
    "                                temp[dataset][temp[dataset]==fill_value]=np.NaN\n",
    "                        except KeyError as e:\n",
    "                            pass\n",
    "                if len(temp) > 0:\n",
    "                    temp['pair']=np.zeros_like(temp['h_li'])+pair\n",
    "                    temp['beam']=np.zeros_like(temp['h_li'])+beam_ind\n",
    "                    temp['filename']=filename\n",
    "                    D6.append(temp)\n",
    "    return D6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9cfe56607d4dcbacd251ac5755002e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f16643e3eb74ef0a85049999f90e8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43407c7935e64b14acd758d30e409566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file in files[0:3]:\n",
    "    this_D6=ATL06_to_dict(file, dataset_dict)\n",
    "    plt.figure()\n",
    "    plt.plot(this_D6[0]['x_atc'], this_D6[0]['h_li'],'.')\n",
    "    plt.title(this_D6[0]['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better!  At least, it's better in that there are not elevations that are wildy unphysical.  The data quality is pretty variable, though.  Take a minute to look through some of the files -- partner up with some of the people around you to find some interesting examples, and we'll get back together in a few minutes.  You can refer to the files you've found by beam number and the count within the list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are some of the files that I picked out as interesting:\n",
    "\n",
    "rough_files=['/home/jovyan/ATL06/Byrd_glacier/processed_ATL06_20181102150421_05340111_209_01.h5', \n",
    "     '/home/jovyan/ATL06/Byrd_glacier/processed_ATL06_20181021060328_03450111_209_01.h5',\n",
    "     '/home/jovyan/ATL06/Byrd_glacier/processed_ATL06_20181112153048_06870111_209_01.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2cebe209ac4923829e47323906f37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D6=ATL06_to_dict(rough_files[0], dataset_dict)\n",
    "markers=['.','+']\n",
    "plt.figure()\n",
    "for ind, seg in enumerate(D6[0:2]):     \n",
    "    plt.plot(seg['x_atc'], seg['h_li'],marker=markers[ind], color='k', linestyle='')\n",
    "    plt.title(seg['filename'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one looks messy!  Segments are scattered between -2km and +2.5 km, and aren't at all consistent with each other.  This one likely has thick clouds that have obscured the surface entirely, and the signal finder has found some random clusters of points with high signal-to-noise ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bede5fa2a4f949178e1c50264276e633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D6=ATL06_to_dict(rough_files[1], dataset_dict)\n",
    "markers=['.','+']\n",
    "plt.figure()\n",
    "for ind, seg in enumerate(D6[0:2]):     \n",
    "    plt.plot(seg['x_atc'], seg['h_li'],marker=markers[ind], color='k', linestyle='')\n",
    "    plt.title(seg['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one looks somewhat better.  There are some good-signal areas mixed in with the bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2906c883884355b4ca594c81135675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D6=ATL06_to_dict(rough_files[2], dataset_dict)\n",
    "markers=['.','+']\n",
    "plt.figure()\n",
    "for ind, seg in enumerate(D6[0:2]):     \n",
    "    plt.plot(seg['x_atc'], seg['h_li'],marker=markers[ind], color='k', linestyle='')\n",
    "    plt.title(seg['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise for participants:\n",
    "Look at the Byrd Glacier profiles.  \n",
    "- Use the matplotlib scatter function to plot the parameters as a function of x_atc and h_li, and see which parameters correspond to good returns, which correspond to clouds.\n",
    "- See which returns are lost if we use the parameter as a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da9b8929fc749ab9db65fbde8740c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in less\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1f393dd4e0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:  \n",
    "dataset_dict['/land_ice_segments/fit_statistics']=['h_rms_misfit']\n",
    "D6=ATL06_to_dict(rough_files[1], dataset_dict)\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.scatter(D6[0]['x_atc'], D6[0]['h_li'], c=D6[0]['h_rms_misfit'], linewidth=0); plt.colorbar()\n",
    "plt.subplot(122)\n",
    "good=np.where(D6[0]['h_rms_misfit'] < .5)[0]\n",
    "plt.plot(D6[0]['x_atc'][good], D6[0]['h_li'][good],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have picked out a set of filtering parameters you like, compare the results with the atl06_quality_summary. Do you have anything you like better?  Regenerate the plot from the top of this section where we mapped the heights for Byrd Glacier using plt.scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more way of flagging bad data that we can try, that can help clean up the remaining bad segments.  Recall that the the ATL06 data model fits both the height and the surface slope of segments: \n",
    "\n",
    "<img src=\"images/dh_segment.tiff\"  width=500 height=450>\n",
    "\n",
    "Let's look at the segments including their slopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_slope_plot(D6):\n",
    "    \"\"\"\n",
    "    Plot a sloping line for each ATL06 segment\n",
    "    \"\"\"\n",
    "    #define the heights of the segment endpoints.  Leave a row of NaNs so that the endpoints don't get joined\n",
    "    h_ep=np.zeros([3, D6['h_li'].size])+np.NaN\n",
    "    h_ep[0, :]=D6['h_li']-D6['dh_fit_dx']*20\n",
    "    h_ep[1, :]=D6['h_li']+D6['dh_fit_dx']*20\n",
    "    # define the x coordinates of the segment endpoints\n",
    "    x_ep=np.zeros([3,D6['h_li'].size])+np.NaN\n",
    "    x_ep[0, :]=D6['x_atc']-20\n",
    "    x_ep[1, :]=D6['x_atc']+20\n",
    "\n",
    "    plt.plot(x_ep.T.ravel(), h_ep.T.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d450be0ccc4da29dbf1cbe1773f474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to try this, we'll need to add dh_fit_dx to the data dictionary:\n",
    "dataset_dict['land_ice_segments/fit_statistics']=['dh_fit_dx']\n",
    "\n",
    "D6=ATL06_to_dict(rough_files[1], dataset_dict)\n",
    "markers=['.','+']\n",
    "plt.figure()\n",
    "for ind, seg in enumerate(D6[0:1]):     \n",
    "    seg_slope_plot(seg)\n",
    "    plt.title(seg['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we zoom in we can see that the \"smooth\" section is a little messy (maybe crevassed), but that the slopes in the cloudy section are really big.  We can use this to define a new filtering strategy that can delete some of the worst segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_difference_filter(D6, tol=2):\n",
    "    \"\"\"\n",
    "    seg_difference_filter: Use elevations and slopes to find bad ATL06 segments\n",
    "    \n",
    "    \n",
    "    Inputs: \n",
    "        D6: a granule of ATL06 data, in dictionary format.  Must have entries:\n",
    "            x_atc, h_li, dh_fit_dx\n",
    "        tol: a tolerance.  Segments whose ends are different from their neighbors \n",
    "             by more than tol are marked as bad\n",
    "    Returns:\n",
    "        good: an array the same size as D6['h_li'].  True  entries indicate that \n",
    "            both ends of the segment are compatible with the segment's neighbors\n",
    "        delta_h_seg: an array the same size as D6['h_li'].  Gives the largest \n",
    "            endpoint difference for each segment    \n",
    "    \"\"\"\n",
    "    h_ep=np.zeros([2, D6['h_li'].size])+np.NaN\n",
    "    h_ep[0, :]=D6['h_li']-D6['dh_fit_dx']*20\n",
    "    h_ep[1, :]=D6['h_li']+D6['dh_fit_dx']*20\n",
    "    delta_h_seg=np.zeros_like(D6['h_li'])\n",
    "    delta_h_seg[1:]=np.abs(D6['h_li'][1:]-h_ep[1, :-1])\n",
    "    delta_h_seg[:-1]=np.maximum(delta_h_seg[:-1], D6['h_li'][:-1]-h_ep[0, 1:])\n",
    "    good=delta_h_seg < tol\n",
    "    return good, delta_h_seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise for participants: Try out the seg_difference_filter.  Generate a finalized map of elevations for Byrd Glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of this part of the tutorial:  \n",
    "- Clouds tend to produce weaker-than-normal returns\n",
    "- We can identify low-quality surface returns with a variety of parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
